# Backend Developer Take-Home Task (Senior Role)  
**Theme: High-Volume Geospatial Telemetry Processing**  

---

## Objective  
Design a **memory-efficient**, **serverless pipeline** to process real-time GPS telemetry data from 50,000+ delivery vehicles. Prioritize **low-latency**, **cost-optimized AWS architecture**, and **scalability to 1M+ daily events**. Use Python, AWS serverless services, and caching layers.  

---

## Mock Dataset  

### **Sample Data Structure**  
- **Telemetry Data** (simulate 10 CSV files with 100k rows each):  
  ```csv  
  vehicle_id,timestamp,lat,lon,speed_kmh,engine_status,fuel_level  
  VH_001,2023-10-10T12:00:00,40.7128,-74.0060,45,moving,75  
  ```  
- **Static Metadata** (`metadata_api.json`):  
  ```json  
  {  
    "VH_001": {"max_speed": 100, "service_region": "NYC", "depot_lat": 40.7128, "depot_lon": -74.0060}  
  }  
  ```  

---
You will have to synthetically generate this data. Try faker or equivalent libraries. 

## Task Sections  

### 1. Basic (Mandatory)  
**Pipeline Architecture & Memory Efficiency**  
- **AWS Serverless Design**:  
  - Propose an architecture using **S3, Lambda, SQS, DynamoDB, and Redis/ElastiCache**.  
  - Justify your choices in `ARCHITECTURE.md` (e.g., "SQS for decoupling producers/consumers").  
- **Data Ingestion**:  
  - Write a Python script to process CSV files in **streaming/chunked mode** (≤100MB chunks).  
  - Validate records (e.g., discard `lat > 90`, `speed > max_speed` from metadata).  
  - **Constraint**: Process 1GB with ≤512MB Lambda memory.  
- **Caching Layer**:  
  - Cache `metadata_api.json` in Redis to reduce API calls. Implement cache invalidation (TTL=1h).  

---

### 2. Advanced 
**Optimization & Fault Tolerance**  
- **Low-Latency Processing**:  
  - Implement **priority queues** in SQS: Process `engine_status="moving"` events before `"idle"`.  
  - Use DynamoDB batch writes to minimize costs.  
- **Memory-Safe Transformations**:  
  - Calculate `distance_from_depot` for each telemetry point using depot coordinates from metadata.  
  - Use **generators/iterators** to avoid loading entire files into memory.  
- **Failure Handling**:  
  - Design retry logic for failed SQS messages (max 3 retries).  
  - Track failures in a Dead Letter Queue (DLQ) with error metadata.  

---

### 3. Exceptional (Leadership/Principal Track)  
**Scalability & Cost Engineering**  
- **Dynamic Scaling**:  
  - Propose a Lambda concurrency auto-scaling mechanism based on SQS queue depth.  
  - Use CloudWatch metrics to trigger scaling (describe in `TRADEOFFS.md`).  
- **Cost-Optimized Storage**:  
  - Tiered S3 storage: Raw telemetry → S3 Standard → Glacier after 7 days.  
  - Convert data to Parquet for archival (explain compression benefits).  
- **Deduplication**:  
  - Deduplicate entries (same `vehicle_id` + `timestamp`) using **Bloom filters** (pseudocode OK).  

---

## Submission Requirements  
1. **Code**:  
   - Python scripts using **zero `pandas`** (stream with `csv.reader`/generators).  
   - Terraform/CDK templates for AWS resources (even if untested).  
   - Unit tests with LocalStack/moto for S3/DynamoDB.  
2. **Documentation**:  
   - `ARCHITECTURE.md`: Diagram + rationale for AWS service choices.  
   - `TRADEOFFS.md`: Explain trade-offs (e.g., "SQS vs Kinesis").  
   - `PERFORMANCE.md`: Benchmark memory usage (e.g., `tracemalloc`).  
3. **AWS Expertise Demonstration**:  
   - If unable to deploy:  
     - Provide **detailed Terraform/CDK templates** with comments.  
     - Write a **failure recovery playbook** (e.g., "How to reprocess DLQ messages").  
     - Explain **cost-saving measures** (e.g., DynamoDB auto-scaling, Lambda memory tuning).  

---

## Evaluation Criteria  
- **Memory Management**:  
  - Use of generators/chunking to avoid OOM errors.  
  - Efficient Redis pipelining for bulk metadata lookups.  
- **Pipeline Efficiency**:  
  - Batching strategies for DynamoDB/SQS.  
  - Priority queue implementation details.  
- **AWS Best Practices**:  
  - Cost-awareness (e.g., Lambda memory-to-CPU ratio).  
  - Observability (structured logs, CloudWatch dashboards).  
- **Senior-Level Insights**:  
  - Preemptive scaling design, deduplication strategies, fault tolerance.  

---

**Time Budget**: 4-6 hours.  
**Deliverable**: Git repo with code + docs. Would be a plus if Senior candidates **could** complete Advanced section.  

---

**Note**:  
- Candidates without AWS access **must** demonstrate expertise through:  
  - Detailed infrastructure diagrams.  
  - Thorough explanations of AWS service interactions (e.g., "How S3 event triggers Lambda").  
  - Cost/performance trade-off analysis.  
- Focus on **code quality**, **system design clarity**, and **defensive programming**.

Submission
Create a private repo and add AISuhasDattatreya and AIYogeshKatria as a reviewer to your PR.
Submit the PR link and the your notes (timelines, breakdown and what you've implented) to suhas@advanced-infrastructure.co.uk and yogesh@advanced-infrastructure.co.uk
We'll go over your code together and discuss your architecture
